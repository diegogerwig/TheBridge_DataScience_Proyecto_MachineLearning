{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limpieza / EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ”Ž Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from ydata_profiling import ProfileReport\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_path='../data/raw/road_traffic_accidents_dataset.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(raw_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = ProfileReport(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ðŸ§¹ Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Remove Duplicates**\n",
    "   - **Objective**: Eliminate duplicate rows in the dataset to avoid redundancy.\n",
    "   - **Action**: Identify and remove duplicate rows based on all or selected columns.\n",
    "   - **Example**: `df.drop_duplicates()`.\n",
    "\n",
    "2. **Handle Missing Values**\n",
    "   - **Objective**: Address missing or incomplete data to ensure data quality.\n",
    "   - **Actions**:\n",
    "     - **Imputation**: Replace missing values with a statistical measure such as mean, median, or mode.\n",
    "     - **Removal**: Remove rows or columns with missing values if they are insignificant.\n",
    "     - **Flagging**: Create a separate flag column to indicate missing values.\n",
    "   - **Example**: `df.fillna()` for imputation or `df.dropna()` to remove missing values.\n",
    "\n",
    "3. **Correct Data Types**\n",
    "   - **Objective**: Ensure that each column in the dataset has the correct data type.\n",
    "   - **Actions**:\n",
    "     - **Conversion**: Convert columns to appropriate data types (e.g., integer, float, datetime).\n",
    "     - **Verification**: Check and verify data types after conversion.\n",
    "   - **Example**: Using pandas, you can use `df.astype()` to change data types.\n",
    "\n",
    "4. **Remove or Address Outliers**\n",
    "   - **Objective**: Identify and handle data points that deviate significantly from other observations.\n",
    "   - **Actions**:\n",
    "     - **Detection**: Use statistical methods or visualization to identify outliers.\n",
    "     - **Handling**: Decide whether to remove outliers, adjust their values, or analyze their impact.\n",
    "   - **Example**: You can use methods like Z-scores or IQR to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# processing_path='../src/data_processing.py'\n",
    "# !python3 {processing_path} {raw_data_path}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data=raw_data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data=temp_data.rename(columns=lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_columns=[]\n",
    "# drop_columns=['Time', 'Day_of_week']\n",
    "temp_data=temp_data.drop(columns=drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data['time']=pd.to_datetime(temp_data['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_encoder = 'accident_severity'\n",
    "label_encoder = LabelEncoder()\n",
    "temp_data[column_encoder] = label_encoder.fit_transform(temp_data[column_encoder])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = pd.get_dummies(temp_data, columns=[col for col in temp_data.columns if col != column_encoder], dtype=int)\n",
    "temp_data=pd.get_dummies(temp_data, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data=temp_data.rename(columns=lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the output file path\n",
    "base_name = os.path.basename(raw_data_path)\n",
    "name, ext = os.path.splitext(base_name)\n",
    "output_file_name = f\"{name}_proc{ext}\"\n",
    "\n",
    "# Define the processed directory path\n",
    "processed_dir = '../data/processed'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "output_path = os.path.join(processed_dir, output_file_name)\n",
    "\n",
    "temp_data.to_csv(output_path, index=False)\n",
    "print(f\"\\nâœ… The processed DataFrame has been saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data_path='../data/processed/road_traffic_accidents_dataset_proc.csv'\n",
    "\n",
    "proc_data=pd.read_csv(proc_data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data.describe(include='all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_data.duplicated().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "injuries = proc_data[['accident_severity']].value_counts()\n",
    "\n",
    "print(injuries)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
